# 100 Days Of ML Code - Activity Log

### Day 0: July 29th, 2018


**Today's Progress**:

Learned about hyperparameter tuning, which entails changing the parameters of a machine learning model, as opposed to making changes to the underlying data.  In this case, was looking at the k-value in a KNN model (see next line).

Worked through exercises to optimize hyperparamters, specifically in a k-nearest-neighbors model (optimizing the k-value by using grid search).

Next, learned about using holdout validation, a process involving splitting your dataset into a 50% train and 50% test set, generating predictions, calculating errors, switching the train and test set, repeating earlier steps, and then averaging your errors.

**Thoughts:** 

The hyperparameter tuning using grid search is pretty quick, easy, and efficient.  I enjoyed learning about it.  However, throughout the exercises, I had to revisit a couple concepts including looping through dictionaries and using the Python built-in function 'enumerate'.

This was my first exposure to holdout validation, which is a pretty handy concept.  I'm looking forward to learning more about this and applying to some other "price prediction" datasets.

**Link to work:** https://github.com/danahagist/100_Days_Of_ML_Code/blob/master/day0_knn.py



